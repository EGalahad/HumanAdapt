--- git status ---
On branch main
Your branch is up to date with 'origin/main'.

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git restore <file>..." to discard changes in working directory)
	modified:   Genesis/genesis/ext/pyrender/viewer.py
	modified:   cfgs/go2.yaml
	modified:   cfgs/leap_hand.yaml
	modified:   envs/reward_wrapper.py
	modified:   envs/vec_env.py
	modified:   train_leaphand.py

Untracked files:
  (use "git add <file>..." to include in what will be committed)
	pose_generate/cfgs/leap_hand/dof_pos copy.yaml
	pose_generate/cfgs/leap_hand/dof_pos.yaml
	record.md

no changes added to commit (use "git add" and/or "git commit -a") 


--- git diff ---
diff --git a/Genesis/genesis/ext/pyrender/viewer.py b/Genesis/genesis/ext/pyrender/viewer.py
index 2275704..1bf5bb8 100644
--- a/Genesis/genesis/ext/pyrender/viewer.py
+++ b/Genesis/genesis/ext/pyrender/viewer.py
@@ -558,6 +558,7 @@ class Viewer(pyglet.window.Window):
             filename = self._get_save_filename(["mp4"])
 
         self.video_recorder.close()
+        print(self.video_recorder.filename, filename)
         shutil.move(self.video_recorder.filename, filename)
 
     def on_close(self):
diff --git a/cfgs/go2.yaml b/cfgs/go2.yaml
index 06c0072..e287995 100644
--- a/cfgs/go2.yaml
+++ b/cfgs/go2.yaml
@@ -40,7 +40,7 @@ learning:
   record_interval: 50
   save_interval: 100
   seed: 1
-  wandb_entity: ziyanx02
+  wandb_entity: elijahgalahad
   wandb_project: Gait
 environment:
   PPO: false
diff --git a/cfgs/leap_hand.yaml b/cfgs/leap_hand.yaml
index 3b44d0b..b86c1b0 100644
--- a/cfgs/leap_hand.yaml
+++ b/cfgs/leap_hand.yaml
@@ -3,7 +3,7 @@ learning:
     class_name: TDO
     clip_param: 0.2
     desired_kl: 0.01
-    entropy_coef: 0.01
+    entropy_coef: 0.001
     gamma: 0.99
     lam: 0.95
     learning_rate: 0.001
@@ -40,7 +40,7 @@ learning:
   record_interval: 50
   save_interval: 100
   seed: 1
-  wandb_entity: ziyanx02
+  wandb_entity: elijahgalahad
   wandb_project: Gait
 environment:
   PPO: false
@@ -69,6 +69,11 @@ environment:
   - dip_2
   - dip_3
   - thumb_dip
+  feet_link_names:
+  - fingertip
+  - fingertip_2
+  - fingertip_3
+  - thumb_fingertip
   body_link_name:
   - base
   PD_stiffness:
@@ -108,32 +113,34 @@ environment:
   termination_if_height_lower_than: 0.0
   termination_if_pitch_greater_than: 0.4
   termination_if_roll_greater_than: 0.4
+
   base_init_pos:
   - 0.0
   - 0.0
-  - 0.24119752645492554
+  - 0.3
+
   base_init_quat:
-  - 0.7071067690849304
+  - 0.70711
+  - 0.0
+  - 0.70711
   - 0.0
-  - 0.7071068286895752
-  - 3.615632238052058e-09
   default_joint_angles:
-    '0': 0.
-    '1': 0.
-    '10': 0.
-    '11': 0.
-    '12': 0.
-    '13': 0.
-    '14': 0.
-    '15': 0.
-    '2': 0.
-    '3': 0.
-    '4': 0.
-    '5': 0.
-    '6': 0.
-    '7': 0.
-    '8': 0.
-    '9': 0.
+    '1': 0.322
+    '5': 0.30301
+    '9': 0.30301
+    '12': 0.41672
+    '0': 0.14064
+    '4': -0.02344
+    '8': 0.10939
+    '13': 1.13867
+    '2': 0.2791
+    '6': 0.20773
+    '10': 0.52891
+    '14': 0.16493
+    '3': 0.11919
+    '7': 0.0
+    '11': 0.00239
+    '15': -0.15052
   resampling_time_s: 4.0
   randomize_base_mass: false
   added_mass_range:
@@ -169,7 +176,7 @@ environment:
   max_push_vel_xy: 1.0
   push_interval_s: -1
   observation:
-    num_obs: 51
+    num_obs: 54
     num_priv_obs: 70
     obs_noise:
       ang_vel: 0.0
@@ -184,4 +191,22 @@ environment:
   reward:
     reward_scales:
       alive: 1.0
-      # reward scales here
+      lin_vel: 1.0
+      ang_vel: 0.5
+      alive: 3.0
+      ang_vel_xy: -0.0001
+      base_height: -0.02
+      dof_acc: -0.01
+      dof_vel: -0.0
+      dof_pos_diff: -0.01
+      lin_vel_z: -0.002
+      orientation: -0.1
+      # contact_force: -1.0
+      # contact_vel: -1.0
+      # feet_height: -10.0
+      # feet_pos: -10.0
+      action_smoothness_1: -0.01
+      action_smoothness_2: -0.01
+      torques: -0.00002
+    soft_dof_pos_limit: 0.9
+    tracking_sigma: 0.25
diff --git a/envs/reward_wrapper.py b/envs/reward_wrapper.py
index 6795c5e..1a5a7a9 100644
--- a/envs/reward_wrapper.py
+++ b/envs/reward_wrapper.py
@@ -8,6 +8,37 @@ from envs.state_wrapper import StateEnv
 class RewardEnv(StateEnv):
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
+
+        self.commands = torch.zeros(self.num_envs, 3, device=self.device)
+        self.commands[:, 0] = 0.5
+        self.commands[:, 1] = 0.0
+        self.commands[:, 2] = 0.0
+
+        self.gait_body_height = 0.25
+        
+        for i, link in enumerate(self.robot.links):
+            print(link.name)
+
+        def find_link_indices(names, accurate=False):
+            link_indices = list()
+            availible = [True for i in range(len(self.robot.links))]
+            for name in names:
+                for i, link in enumerate(self.robot.links):
+                    if availible[i] and (accurate==False and name in link.name or name == link.name):
+                        availible[i] = False
+                        link_indices.append(link.idx - self.robot.link_start)
+            return link_indices
+
+        self.termination_contact_link_indices = find_link_indices(
+            self.env_cfg['termination_contact_link_names']
+        )
+        self.penalized_contact_link_indices = find_link_indices(
+            self.env_cfg['penalized_contact_link_names']
+        )
+        self.feet_link_indices = find_link_indices(
+            self.env_cfg['feet_link_names'],
+            accurate=True,
+        )
     
     def _reward_alive(self):
         """
@@ -21,4 +52,188 @@ class RewardEnv(StateEnv):
     # For reward function introduced as REWARD_FUNC in the config file,
     # the function name should be _reward_REWARD_FUNC
     # def _reward_REWARD_FUNC(self):
-    #     return torch.zeros(self.num_envs, device=self.device)
\ No newline at end of file
+    #     return torch.zeros(self.num_envs, device=self.device)
+
+    def _reward_lin_vel(self):
+        """
+        Reward for tracking linear velocity commands (x, y axes).
+        - Computes the squared error between the desired linear velocity (commands[:, :2]) and the actual body linear velocity (body_lin_vel[:, :2]).
+        - Applies an exponential decay to the error to encourage smooth tracking.
+        - Higher reward for smaller errors.
+        """
+        lin_vel_error = torch.norm(self.commands[:, :2] - self.body_lin_vel[:, :2], dim=-1)
+        return torch.exp(-lin_vel_error / 0.25)
+
+    def _reward_ang_vel(self):
+        """
+        Reward for tracking angular velocity commands (yaw axis).
+        - Computes the squared error between the desired angular velocity (commands[:, 2]) and the actual body angular velocity (body_ang_vel[:, 2]).
+        - Applies an exponential decay to the error to encourage smooth tracking.
+        - Higher reward for smaller errors.
+        """
+        ang_vel_error = torch.abs(self.commands[:, 2] - self.body_ang_vel[:, 2])
+        return torch.exp(-ang_vel_error / 0.25)
+
+    def _reward_base_height(self):
+        """
+        Reward for maintaining the desired body height.
+        - Computes the squared error between the current body height (body_pos[:, 2]) and the desired gait body height (gait_body_height).
+        - Encourages the robot to maintain a stable body height during locomotion.
+        """
+        return torch.square(self.body_pos[:, 2] - self.gait_body_height)
+
+    def _reward_contact_force(self):
+        """
+        Reward for tracking desired contact forces on the feet.
+        - Computes the norm of contact forces on the feet (link_contact_forces) and compares them to the desired contact states (desired_contact_states).
+        - Encourages feet to exert force when in contact and minimize force when not in contact.
+        - Uses an exponential decay to penalize large forces when contact is not desired.
+        """
+        foot_forces = torch.norm(self.link_contact_forces[:, self.feet_link_indices, :], dim=-1)
+        desired_contact = self.desired_contact_states
+        return torch.mean((1 - desired_contact) * (1 - torch.exp(-foot_forces ** 2 / 100.)), dim=-1)
+    
+    def _reward_feet_height(self):
+        """
+        Reward for tracking desired feet height.
+        - Computes the squared error between the current feet height (feet_pos_local[..., 2]) and the desired feet height (desired_feet_pos_local[..., 2]).
+        - Only applies the reward when the feet are not in contact (1 - desired_contact_states).
+        - Encourages the robot to lift its feet to the correct height during swing phases.
+        """
+        rew_foot_height = torch.square(self.feet_pos_local[..., 2] - self.desired_feet_pos_local[..., 2]) * (1 - self.desired_contact_states)
+        return torch.mean(rew_foot_height, dim=-1)
+    
+    def _reward_feet_pos(self):
+        """
+        Reward for tracking desired feet position (x, y axes).
+        - Computes the squared error between the current feet position (feet_pos_local[..., 0:2]) and the desired feet position (desired_feet_pos_local[..., 0:2]).
+        - Encourages the robot to place its feet accurately in the x-y plane.
+        """
+        rew_foot_pos = torch.sum(torch.square(self.feet_pos_local[..., 0:2] - self.desired_feet_pos_local[..., 0:2]), dim=-1)
+        return torch.mean(rew_foot_pos, dim=-1)
+
+    def _reward_alive(self):
+        """
+        Reward for staying alive (not terminating).
+        - Returns 1 if the robot is alive (terminate_buf is False), otherwise 0.
+        - Encourages the robot to avoid conditions that lead to termination (e.g., falling).
+        """
+        return 1 - self.terminate_buf.float()
+
+    def _reward_terminate(self):
+        """
+        Penalty for termination.
+        - Returns 1 if the robot has terminated (terminate_buf is True), otherwise 0.
+        - Discourages behaviors that lead to termination.
+        """
+        return self.terminate_buf.float()
+
+    # Regularization terms
+    # These rewards penalize undesirable behaviors to encourage smooth, stable, and efficient locomotion.
+
+    def _reward_collision(self):
+        """
+        Penalty for undesired collisions on specific body links.
+        - Penalizes contact forces on links that shouldn't touch the ground in stable poses.
+        - Especially important when intermediate poses might involve brief contact—this reward discourages such transient but undesirable behaviors.
+        """
+        return torch.sum(torch.norm(self.link_contact_forces[:, self.penalized_contact_link_indices, :], dim=-1,) > 0.1, dim=1)
+
+    def _reward_lin_vel_z(self):
+        """
+        Penalty for body linear velocity in the z-axis.
+        - Computes the squared z-axis linear velocity (body_lin_vel[:, 2]).
+        - Discourages unnecessary vertical motion of the body.
+        """
+        return torch.square(self.body_lin_vel[:, 2])
+    
+    def _reward_ang_vel_xy(self):
+        """
+        Penalty for body angular velocity in the x and y axes.
+        - Computes the squared angular velocity in the x and y axes (body_ang_vel[:, :2]).
+        - Discourages unnecessary tilting or rolling of the body.
+        """
+        return torch.sum(torch.square(self.body_ang_vel[:, :2]), dim=-1)
+
+    def _reward_orientation(self):
+        """
+        Penalty for non-flat body orientation.
+        - Computes the squared deviation of the projected gravity vector from the vertical axis (body_projected_gravity[:, :2]).
+        - Encourages the robot to maintain a flat body orientation.
+        """
+        return torch.sum(torch.square(self.body_projected_gravity[:, :2]), dim=-1)
+
+    def _reward_torques(self):
+        """
+        Penalty for high joint torques.
+        - Computes the squared torques applied to the joints (torques).
+        - Encourages energy-efficient locomotion by minimizing joint torques.
+        """
+        return torch.mean(torch.square(self.torques), dim=-1)
+
+    def _reward_dof_vel(self):
+        """
+        Penalty for high joint velocities.
+        - Computes the squared velocities of the degrees of freedom (dof_vel).
+        - Encourages smooth and controlled joint movements.
+        """
+        return torch.mean(torch.square(self.dof_vel), dim=-1)
+
+    def _reward_dof_acc(self):
+        """
+        Penalty for high joint accelerations.
+        - Computes the squared accelerations of the degrees of freedom (dof_vel changes over time).
+        - Encourages smooth transitions in joint movements.
+        """
+        return torch.mean(torch.square((self.last_dof_vel - self.dof_vel)), dim=-1)
+
+    def _reward_dof_pos_diff(self):
+        """
+        Penalty for deviations from the default joint positions.
+        - Computes the squared difference between current joint positions (dof_pos) and default joint positions (default_dof_pos).
+        - Encourages the robot to maintain a natural pose.
+        """
+        return torch.mean(torch.square(self.dof_pos - self.default_dof_pos), dim=-1)
+
+    def _reward_contact_vel(self):
+        """
+        Penalty for high vertical velocities during contact.
+        - Computes the norm of foot velocities (foot_velocities) and penalizes large vertical velocities when feet are in contact (desired_contact_states).
+        - Encourages stable foot placement during contact phases.
+        """
+        foot_velocities = torch.norm(self.foot_velocities, dim=2).view(self.num_envs, -1)
+        desired_contact = self.desired_contact_states
+        return torch.mean(desired_contact * (1 - torch.exp(-foot_velocities ** 2 / 10.)), dim=-1)
+
+    def _reward_action_smoothness_1(self):
+        """
+        Penalty for 1st-order action deviations.
+        - Computes the squared difference between current actions (actions) and previous actions (last_actions).
+        - Encourages smooth transitions between actions.
+        """
+        diff = torch.square(self.actions - self.last_actions)
+        diff = diff * (self.last_actions != 0)  # ignore first step
+        return torch.mean(diff, dim=-1)
+
+    def _reward_action_smoothness_2(self):
+        """
+        Penalty for 2nd-order action deviations.
+        - Computes the squared difference between current actions (actions), previous actions (last_actions), and actions before that (last_last_actions).
+        - Encourages even smoother transitions between actions.
+        """
+        diff = torch.square(self.actions - 2 * self.last_actions + self.last_last_actions)
+        diff = diff * (self.last_actions != 0) * (self.last_last_actions != 0)  # ignore first & second steps
+        return torch.mean(diff, dim=-1)
+
+    def _reward_dof_pos_limits(self):
+        """
+        Penalty for dof positions too close to the limit 
+        - Compute the difference between current dof positions and dof pos limits.
+        - Encourage proper dof position to avoid extreme pose.
+        - Critical for deploying in real.
+        """
+        # Penalize dof positions too close to the limit
+        out_of_limits = -(self.dof_pos - self.dof_pos_limits[:, 0]).clip(max=0.0)  # lower limit
+        out_of_limits += (self.dof_pos - self.dof_pos_limits[:, 1]).clip(min=0.0)  # upper limit
+        return torch.sum(out_of_limits, dim=1) # >=0
+    
\ No newline at end of file
diff --git a/envs/vec_env.py b/envs/vec_env.py
index d2a582d..26d1ce0 100644
--- a/envs/vec_env.py
+++ b/envs/vec_env.py
@@ -684,6 +684,7 @@ class VecEnv:
     def compute_observation(self):
         obs_buf = torch.cat(
             [
+                self.body_lin_vel * self.obs_scales['lin_vel'],                     # 3
                 self.body_ang_vel_local * self.obs_scales['ang_vel'],               # 3
                 (self.dof_pos - self.default_dof_pos) * self.obs_scales['dof_pos'],
                 self.dof_vel * self.obs_scales['dof_vel'],                         
diff --git a/train_leaphand.py b/train_leaphand.py
index b68eba0..ba91cdf 100644
--- a/train_leaphand.py
+++ b/train_leaphand.py
@@ -91,7 +91,7 @@ if __name__ == '__main__':
     parser.add_argument('-v', '--vis', action='store_true', default=False)
     parser.add_argument('-c', '--cpu', action='store_true', default=False)
     parser.add_argument('-B', '--num_envs', type=int, default=4096)
-    parser.add_argument('--max_iterations', type=int, default=1000)
+    parser.add_argument('--max_iterations', type=int, default=5000)
     parser.add_argument('--resume', type=str, default=None)
     parser.add_argument('-o', '--offline', action='store_true', default=False)
     parser.add_argument('--time', action='store_true', default=False)